 %% [markdown]
# # AIML Project — Iris Classification (Week 1–3)
# 
# **Student**: Swati Chaudhari  
# **Dataset:** Iris Dataset  
# **Goal:** Predict Iris flower species based on sepal & petal measurements.
# 
# ---
# 

# %%
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix


# %%
# Load dataset
df = pd.read_csv(r"C:\Users\kumar\Documents\AIML_Project\data\iris.csv")
print("Dataset shape:", df.shape)

# Show first rows
df.head()


# %% [markdown]
# WEEK 1- Load and explore dataset
# 
# ## Week 1 — Problem Statement
# 
# - The task is to predict the species of iris flowers based on features:
#   - Sepal length
#   - Sepal width
#   - Petal length
#   - Petal width
# 
# - Target variable: `species`
# 
# 

# %%
print(df.info())
print(df.describe())
print("Class distribution:\n", df['species'].value_counts())


# %%
# Load dataset
df = pd.read_csv(r"C:\Users\kumar\Documents\AIML_Project\data\iris.csv")
print("Dataset shape:", df.shape)

# Show first rows
df.head()


# %%
# Load a CSV file into a DataFrame
df = pd.read_csv(r"C:\Users\kumar\Documents\AIML_Project\data\iris.csv")

# Display the first 5 rows
print(df.head())

# Get a summary of the DataFrame, including data types and non-null values
print(df.info())

# Show descriptive statistics for numerical columns
print(df.describe())


# %%
sns.pairplot(df, hue="species")
plt.show()


# %% [markdown]
# week 2- data preprocessing
# 

# %% [markdown]
# //handling the missing values

# %%
# Count missing values per column
# Import required libraries
from sklearn.impute import SimpleImputer
import pandas as pd

# Check missing values in each column
print("Missing values in each column:")
print(df.isnull().sum())

# Define numerical and categorical columns
numerical_cols = ['sepal length (cm)', 'sepal width (cm)', 
                 'petal length (cm)', 'petal width (cm)']
categorical_cols = ['species']

# Handle numerical missing values
if df[numerical_cols].isnull().sum().any():
    # Using mean imputation for numerical columns
    num_imputer = SimpleImputer(strategy='mean')
    df[numerical_cols] = num_imputer.fit_transform(df[numerical_cols])

# Handle categorical missing values
if df[categorical_cols].isnull().sum().any():
    # Using mode imputation for categorical columns
    cat_imputer = SimpleImputer(strategy='most_frequent')
    df[categorical_cols] = cat_imputer.fit_transform(df[categorical_cols])
# Verify no missing values remain
print("\nMissing values after imputation:")
print(df.isnull().sum())

# Display dataset shape
print("\nDataset shape:", df.shape)

# %% [markdown]
# // Encoding categorial data 

# %%
# Import necessary libraries for encoding
from sklearn.preprocessing import OneHotEncoder, LabelEncoder


# One-hot encode the 'species' column using Pandas
df_encoded = pd.get_dummies(df, columns=['species'], drop_first=True)

# Alternative: Using Scikit-learn for one-hot encoding
encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
encoded_species = encoder.fit_transform(df[['species']])
encoded_species_df = pd.DataFrame(
    encoded_species, 
    columns=encoder.get_feature_names_out(['species'])
)

# Label encoding (if needed)
label_encoder = LabelEncoder()
df['species_encoded'] = label_encoder.fit_transform(df['species'])

# Display results
print("One-hot encoded data (first 5 rows):")
print(df_encoded.head())
print("\nLabel encoded data (first 5 rows):")
print(df[['species', 'species_encoded']].head())

# %% [markdown]
# // handling outliiers
# 

# %%
# Import required libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Define numerical columns to check for outliers
numerical_cols = ['sepal length (cm)', 'sepal width (cm)', 
                 'petal length (cm)', 'petal width (cm)']

# Dictionary to store outliers for each column
outliers_dict = {}

# Detect outliers for each numerical column
for column in numerical_cols:
    # Calculate quartiles and IQR
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    
    # Calculate bounds
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    # Find outliers
    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]
    outliers_dict[column] = len(outliers)

# Display outlier information
for column, count in outliers_dict.items():
    print(f"\nOutliers in {column}: {count}")

# Visualize outliers with boxplot
plt.figure(figsize=(10, 6))
sns.boxplot(data=df[numerical_cols])
plt.title('Distribution of Features with Outliers')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# %%
X = df.drop(columns="species")
y = df["species"]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print("Train size:", X_train.shape)
print("Test size:", X_test.shape)


# %% [markdown]
# ## Week 2 — Model Selection
# 
# We choose **RandomForestClassifier** because:
# - It handles classification well
# - Works with small datasets
# - Provides feature importance
# 

# %%
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)


# %%
print("✅ Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))


# %% [markdown]
# Week 2: Confusion Matrix

# %%
cm = confusion_matrix(y_test, y_pred, labels=model.classes_)

plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, fmt="d", xticklabels=model.classes_, yticklabels=model.classes_, cmap="Blues")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.tight_layout()
plt.savefig("fig_confusion_matrix.png", dpi=150)  # saved for PPT
plt.show()


# %% [markdown]
# Week 3: Deployment Simulation

# %%
# Example prediction
sample = [[5.1, 3.5, 1.4, 0.2]]  # Sepal length, sepal width, petal length, petal width
prediction = model.predict(sample)
print("Predicted Species:", prediction[0])


# %%
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Read the dataset
df = pd.read_csv(r"C:\Users\kumar\Documents\AIML_Project\data\iris.csv")

# Display first 5 rows
print("First 5 rows of the dataset:")
print(df.head())

# Display basic information about the dataset
print("\nDataset Information:")
print(df.info())

# Display summary statistics
print("\nSummary Statistics:")
print(df.describe())

# %% [markdown]
# Week 3- Deployement process of ml model

# %%
import joblib
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification

# 1. Train your model
X, y = make_classification(n_samples=100, n_features=4, random_state=42)
model = RandomForestClassifier()
model.fit(X, y)

# 2. Save the trained model to a file
joblib.dump(model, 'random_forest_model.joblib')


# %% [markdown]
#  Create a web API with a Python framework

# %%
from fastapi import FastAPI
from pydantic import BaseModel
import joblib
import numpy as np

# Define the structure of your incoming data
class PredictionInput(BaseModel):
    features: list[float]

# Initialize the FastAPI app
app = FastAPI()

# Load the trained model
model = joblib.load('random_forest_model.joblib')

# Create an API endpoint for predictions
@app.post("/predict")
def predict(data: PredictionInput):
    input_array = np.array(data.features).reshape(1, -1)
    prediction = model.predict(input_array)
    return {"prediction": int(prediction[0])}


# %% [markdown]
# # How to build and run the Docker container
# """
# # Open terminal in project directory and run:
# docker build -t iris-classifier .
# docker run -p 8000:8000 iris-classifier
# 
# # Access the API at:
# http://localhost:8000/docs
# """

# %%


# %%
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Configure pandas display options
pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)
pd.set_option('display.width', None)
pd.set_option('display.max_colwidth', None)

# Read the dataset
df = pd.read_csv(r"C:\Users\kumar\Documents\AIML_Project\data\iris.csv")

# Display first 5 rows with full output
print("First 5 rows of the dataset:")
print(df.head().to_string())

# Display basic information about the dataset
print("\nDataset Information:")
print(df.info())

# Display summary statistics with full output
print("\nSummary Statistics:")
print(df.describe().to_string())



# %%
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix


# %%
df = pd.read_csv(r"C:\Users\kumar\Documents\AIML_Project\data\iris.csv")
print("Dataset shape:", df.shape)
df.head()


# %%
# Data exploration and visualization
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load and verify dataset
df = pd.read_csv(r"C:\Users\kumar\Documents\AIML_Project\data\iris.csv")
print(f"Dataset shape: {df.shape}")

# Create visualizations
plt.figure(figsize=(10, 6))
sns.boxplot(data=df.drop('species', axis=1))
plt.title('Distribution of Iris Features')
plt.xticks(rotation=45)
plt.show()

# Display summary statistics
print("\nSummary Statistics:")
print(df.describe())

# Display class distribution
print("\nClass Distribution:")
print(df['species'].value_counts())


